---
title: 'Academic: the website builder for Hugo'
subtitle: 'Create a beautifully simple website in under 10 minutes :rocket:'
summary: DrNAS: Dirichlet Neural Architecture Search.
authors:
- admin
- å³æ©é”
tags:
- Academic
- å¼€æº
categories:
- Demo
- æ•™ç¨‹
date: "2016-04-20T00:00:00Z"
lastmod: "2019-04-17T00:00:00Z"
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Placement options: 1 = Full column width, 2 = Out-set, 3 = Screen-width
# Focal point options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
image:
  placement: 2
  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/CpkOjOcXdUY)'
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
---

**Create a free website with Academic using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 _widgets_, _themes_, and _language packs_ included!**

[Check out the latest **demo**](https://academic-demo.netlify.com/) of what you'll get in less than 10 minutes, or [view the **showcase**](https://sourcethemes.com/academic/#expo) of personal, project, and business sites.

- ğŸ‘‰ [**Get Started**](#install)
- ğŸ“š [View the **documentation**](https://sourcethemes.com/academic/docs/)
- ğŸ’¬ [**Ask a question** on the forum](https://discourse.gohugo.io)
- ğŸ‘¥ [Chat with the **community**](https://spectrum.chat/academic)
- ğŸ¦ Twitter: [@source_themes](https://twitter.com/source_themes) [@GeorgeCushen](https://twitter.com/GeorgeCushen) [#MadeWithAcademic](https://twitter.com/search?q=%23MadeWithAcademic&src=typd)
- ğŸ’¡ [Request a **feature** or report a **bug**](https://github.com/gcushen/hugo-academic/issues)
- â¬†ï¸ **Updating?** View the [Update Guide](https://sourcethemes.com/academic/docs/update/) and [Release Notes](https://sourcethemes.com/academic/updates/)
- :heart: **Support development** of Academic:
  - â˜•ï¸ [**Donate a coffee**](https://paypal.me/cushen)
  - ğŸ’µ [Become a backer on **Patreon**](https://www.patreon.com/cushen)
  - ğŸ–¼ï¸ [Decorate your laptop or journal with an Academic **sticker**](https://www.redbubble.com/people/neutreno/works/34387919-academic)
  - ğŸ‘• [Wear the **T-shirt**](https://academic.threadless.com/)
  - :woman_technologist: [**Contribute**](https://sourcethemes.com/academic/docs/contribute/)

{{< figure src="https://raw.githubusercontent.com/gcushen/hugo-academic/master/academic.png" title="Academic is mobile first with a responsive design to ensure that your site looks stunning on every device." >}}

**Key features:**

- **Page builder** - Create *anything* with [**widgets**](https://sourcethemes.com/academic/docs/page-builder/) and [**elements**](https://sourcethemes.com/academic/docs/writing-markdown-latex/)
- **Edit any type of content** - Blog posts, publications, talks, slides, projects, and more!
- **Create content** in [**Markdown**](https://sourcethemes.com/academic/docs/writing-markdown-latex/), [**Jupyter**](https://sourcethemes.com/academic/docs/jupyter/), or [**RStudio**](https://sourcethemes.com/academic/docs/install/#install-with-rstudio)
- **Plugin System** - Fully customizable [**color** and **font themes**](https://sourcethemes.com/academic/themes/)
- **Display Code and Math** - Code highlighting and [LaTeX math](https://en.wikibooks.org/wiki/LaTeX/Mathematics) supported
- **Integrations** - [Google Analytics](https://analytics.google.com), [Disqus commenting](https://disqus.com), Maps, Contact Forms, and more!
- **Beautiful Site** - Simple and refreshing one page design
- **Industry-Leading SEO** - Help get your website found on search engines and social media
- **Media Galleries** - Display your images and videos with captions in a customizable gallery
- **Mobile Friendly** - Look amazing on every screen with a mobile friendly version of your site
- **Multi-language** - 15+ language packs including English, ä¸­æ–‡, and PortuguÃªs
- **Multi-user** - Each author gets their own profile page
- **Privacy Pack** - Assists with GDPR
- **Stand Out** - Bring your site to life with animation, parallax backgrounds, and scroll effects
- **One-Click Deployment** - No servers. No databases. Only files.

## Themes

Academic comes with **automatic day (light) and night (dark) mode** built-in. Alternatively, visitors can  choose their preferred mode - click the sun/moon icon in the top right of the [Demo](https://academic-demo.netlify.com/) to see it in action! Day/night mode can also be disabled by the site admin in `params.toml`.

[Choose a stunning **theme** and **font**](https://sourcethemes.com/academic/themes/) for your site. Themes are fully [customizable](https://sourcethemes.com/academic/docs/customization/#custom-theme).

## Ecosystem

* **[Academic Admin](https://github.com/sourcethemes/academic-admin):** An admin tool to import publications from BibTeX or import assets for an offline site
* **[Academic Scripts](https://github.com/sourcethemes/academic-scripts):** Scripts to help migrate content to new versions of Academic

## Install

You can choose from one of the following four methods to install:

* [**one-click install using your web browser (recommended)**](https://sourcethemes.com/academic/docs/install/#install-with-web-browser)
* [install on your computer using **Git** with the Command Prompt/Terminal app](https://sourcethemes.com/academic/docs/install/#install-with-git)
* [install on your computer by downloading the **ZIP files**](https://sourcethemes.com/academic/docs/install/#install-with-zip)
* [install on your computer with **RStudio**](https://sourcethemes.com/academic/docs/install/#install-with-rstudio)

Then [personalize and deploy your new site](https://sourcethemes.com/academic/docs/get-started/).

## Updating

[View the Update Guide](https://sourcethemes.com/academic/docs/update/).

Feel free to *star* the project on [Github](https://github.com/gcushen/hugo-academic/) to help keep track of [updates](https://sourcethemes.com/academic/updates).

## License

Copyright 2016-present [George Cushen](https://georgecushen.com).

Released under the [MIT](https://github.com/gcushen/hugo-academic/blob/master/LICENSE.md) license.

æœ¬æ–‡ä»æ¨¡å‹æœç´¢**NAS**çš„é—®é¢˜å‡ºå‘ï¼Œæ•´ç†äº†æœ€æ–°**ICLR2021**ç›¸å…³æŠ•ç¨¿è®ºæ–‡ã€‚ ç¥ç»ç½‘ç»œé™¤äº†æƒé‡**(W)**ä¹‹å¤–ï¼Œå…¶é€šé“æ•°ï¼Œç®—å­ç±»å‹å’Œç½‘ç»œè¿æ¥ç­‰ç»“æ„å‚æ•°éœ€è¦è®¾å®šï¼Œè€Œæ¨¡å‹æœç´¢NASå³æ˜¯ç¡®å®šç»“æ„å‚æ•°çš„è‡ªåŠ¨æ–¹æ³•ã€‚æœ€åˆ**NASNet**ä¸­æ¯ç§ç»“æ„å‚æ•°çš„æ¨¡å‹å•ç‹¬è®­ç»ƒå¸¦æ¥çš„å·¨å¤§å¼€é”€ï¼Œæœ€è¿‘ä¸¤å¹´åŸºäºæƒé‡å…±äº«çš„NASæ–¹æ³•ä¸­ï¼Œä¸åŒç»“æ„å‚æ•°æ¨¡å‹å¤ç”¨æƒé‡ç»„æˆä»£ç†æ¨¡å‹**(SuperNet)**ä¸€èµ·è®­ç»ƒï¼Œç„¶åè¯„æµ‹å­æ¨¡å‹æŒ‡æ ‡å¹¶é€šè¿‡**RL , EA , Random**æœç´¢**(One-shot)**æˆ–ç”±å‚æ•°åŒ–ç¦»æ•£å˜è¿ç»­ç”¨æ¢¯åº¦ä¸‹é™**(Darts)**ä»ç»“æ„å‚æ•°ç©ºé—´**(A)**æ±‚è§£å‡ºæœ€ä¼˜å­ç»“æ„ï¼Œæœ€åé‡è®­æœ€ä¼˜å­ç»“æ„å¾—æœ€åéœ€è¦çš„æ¨¡å‹ã€‚æ•´ä¸ªæµç¨‹ä¸­åˆ†ä¸º**SuperNet**è®­ç»ƒï¼Œæœ€ä¼˜å­æ¨¡å‹æœç´¢ï¼Œé‡è®­ä¸‰ä¸ªé˜¶æ®µï¼Œå…¶ä¸­æœç´¢é˜¶æ®µæ—¶é—´å› ä¸ºä¸åŒçš„è¯„æµ‹æ–¹å¼å’ŒæŒ‡æ ‡ï¼Œå¿«åˆ™å‡ ç§’æ…¢åˆ™å‡ å¤©ï¼Œè€Œ**SuperNet**è®­ç»ƒå‘¨æœŸä¸€èˆ¬è®¾ç½®æˆé‡è®­é˜¶æ®µç›¸è¿‘ï¼Œå› æ­¤ç›®å‰æµè¡Œçš„æƒé‡å…±äº«æœç´¢æ–¹æ³•å¤šæ˜¯å•ç‹¬è®­ç»ƒçš„ä¸¤å€å·¦å³å¼€é”€ã€‚å…¶ä¸­ç»“æ„å‚æ•°ç©ºé—´å¦‚ä½•å»ºæ¨¡ï¼Œä»£ç†æ¨¡å‹è¯„æµ‹å¥½åæ˜¯å¦çœŸå®(ä¸€è‡´æ€§)ï¼Œä»¥åŠè®­ç»ƒå¼€é”€æ˜¯å¦å¯ä»¥è¿›ä¸€æ­¥é™ä½ï¼Œè¿™äº›é—®é¢˜å¯¹åº”æŠ•ç¨¿è®ºæ–‡æ•´ç†å¦‚ä¸‹ï¼š

- **One-shotæ–¹æ³•ä¸­SuperNetè®­ç»ƒä»¥åŠä¸€è‡´æ€§ï¼Ÿ**  One-shotæ–¹æ³•ä¸­å¤§å¤šä»¥megvii çš„Singlepathä¸ºframeworkï¼Œä¹‹åçš„æ”¹è¿›å·¥ä½œä¸»è¦é›†ä¸­åœ¨é‡‡æ ·æ–¹å¼å’Œå…·ä½“è®­ç»ƒç­–ç•¥ä¸Šï¼ŒGreedyNAS å’Œ AngleNASåˆ†åˆ«ç”¨droppathå’Œdropnodeæ”¹è¿›é‡‡æ ·æ–¹å¼ï¼ŒOnce for allå’ŒBigNASåˆ©ç”¨è®­ç»ƒç­–ç•¥ä½¿å¾—SuperNetä¸­å­æ¨¡å‹æ€§èƒ½å˜å¼ºè€Œçœå»äº†é‡è®­æ­¥éª¤ï¼Œæ–‡ã€1ã€‘æ˜¯å¹´åˆçš„æ–‡ç« æ€»ç»“äº†è®­ç»ƒç»†èŠ‚æ¯”å¦‚åˆ†ç»„BNç­‰å½±å“ã€‚ One-shotæ¡†æ¶ä¸­å…¶ä»–é—®é¢˜å¦‚ä»£ç†æ¨¡å‹è¯„æµ‹è¯¯å·®ç­‰ä¹Ÿéƒ½æœ‰ICLR2021æŠ•ç¨¿å·¥ä½œç ”ç©¶ï¼›
- **Dartsæ–¹æ³•çš„è®­ç»ƒå’Œä¼˜åŒ–æ–¹å¼ï¼Ÿ**  Dartsæ–¹æ³•ç”¨bi-levelä¼˜åŒ–è½®æ›¿ä¼˜åŒ–æƒé‡(W)å’Œç»“æ„å‚æ•°(A)ï¼Œå› ä¸ºsoftmaxä»¥åŠæ— å‚æ•°OPå½±å“å¯¼è‡´æ¨¡å‹åå¡Œä»¥åŠç»“æœä¸ç¨³å®šï¼Œæ–‡ã€2ã€‘é€šè¿‡å¼•å…¥è¾…åŠ©åˆ†æ”¯æ”¹å–„è®­ç»ƒï¼Œæ–‡ã€3ã€‘åŠ noiseæ”¹å–„äº†æ¨¡å‹åå¡Œï¼Œæ–‡ã€4ã€‘å’Œæ–‡ã€5ã€‘éƒ½æ˜¯è§£è€¦è¿æ¥å’ŒOPçš„æœç´¢ã€‚ç”±äºbi-levelæ±‚è§£å­˜åœ¨ä¼˜åŒ–è¯¯å·®æ–‡ï¼Œæ–‡ã€6ã€‘å’Œæ–‡ã€7ã€‘é‡‡ç”¨single-levelä¼˜åŒ–æ–¹æ³•ï¼Œå¸Œæœ›è¿™ä¸¤ç¯‡å·¥ä½œèƒ½ç»“æŸé­”æ”¹Dartsçš„å±€é¢ï¼›
- **ç»“æ„å‚æ•°ç©ºé—´æ€ä¹ˆå»ºæ¨¡ï¼Ÿ**æ¯”å¦‚æ–‡ã€8ã€‘æŒ‰ç…§åˆ†å¸ƒå»ºæ¨¡ï¼Œæ–‡ã€9ã€‘æŒ‰ç…§æµå½¢å»ºæ¨¡ï¼Œæ–‡ã€10ã€‘æŒ‰ç…§é‚»æ¥å…³ç³»å»ºæ¨¡ï¼Œéšç€NAS [benchmarks](https://github.com/automl/nas_benchmarks)101ç­‰å‡ºç°ï¼Œç›´æ¥ç”¨ç»“æ„å‚æ•°ä½œä¸ºè¾“å…¥Xï¼Œæå‰æµ‹è¯•å¥½çš„å¯¹åº”ç²¾åº¦ä½œä¸ºY, å­¦ä¹ Xåˆ°Yæ˜ å°„å…³ç³»å¯ä»¥çœ‹ä½œå¯¹æœç´¢ç©ºé—´å»ºæ¨¡ï¼Œè¿™æ ·å·¥ä½œæœ‰åŸºäºGCN, LSTMç­‰çš„é¢„æµ‹å™¨ä»¥åŠæ’åºå™¨ç­‰ï¼Œå¦‚æ–‡ã€6ã€‘ï¼Œæ–‡ã€7ã€‘å’Œæ–‡ã€11ã€‘ç­‰ï¼›
- **NAS without training?** æ–‡ã€13ã€‘å’Œ æ–‡ã€14ã€‘ç±»ä¼¼ï¼Œä»¥åŠæˆ‘ä»¬æ‰€æ›´æ—©çš„å·¥ä½œModuleNetï¼Œéƒ½æ˜¯é€šè¿‡ç²¾å¿ƒè®¾è®¡æŒ‡æ ‡å–ä»£ç²¾åº¦æ¥é¿å…æƒé‡è®­ç»ƒçš„å¼€é”€ï¼Œè™½ç„¶ç»“æœå›¾æœ‰ç›¸å…³è¶‹åŠ¿ï¼Œä½†æ˜¯ç»å®é™…è¯„æµ‹ä¸€è‡´æ€§æ•°å€¼è¾ƒä½ï¼Œå…¶ä»–å¦‚éšæœºæƒé‡çš„å·¥ä½œåŸºæœ¬åªèƒ½è§£å†³å¾ˆç®€å•ä»»åŠ¡éš¾ä»¥å®ç”¨ï¼›
- **æ–°çš„NAS** [**benchmark**](https://github.com/automl/nas_benchmarks)**?** å¦‚ [TransNAS-Bench-101](https://openreview.net/forum?id=HUd2wQ0j200)ï¼Œ[NAS-Bench-301](https://openreview.net/forum?id=1flmvXGGJaa)ï¼Œ[HW-NAS-Bench](https://openreview.net/forum?id=_0kaDkv3dVf)ç­‰ï¼Œéƒ½æ˜¯æ¨åŠ¨é¢†åŸŸå¤§å¤§çš„å¥½å·¥ä½œï¼Œrespectï¼
- **NASçš„ä¸‹æ¸¸ä»»åŠ¡åº”ç”¨ï¼Ÿ**æ£€æµ‹ä¸Šåº”ç”¨å¦‚æ–‡ã€16ã€‘, é‡åŒ–ä¸Šåº”ç”¨å¦‚æ–‡ã€17ã€‘ç­‰ã€‚

**æ€»ç»“ï¼š**48ç¯‡æ–‡ç« ä¸­æ—¢æœ‰å»¶ç»­ä¹‹å‰è§£å†³One-shotå’ŒDartsæ¡†æ¶ä¸­è®­ç»ƒä¸ç¨³å®šä»¥åŠå‡å°‘è¯„æµ‹è¯¯å·®ç­‰æ–¹é¢é—®é¢˜çš„æ–‡ç« ï¼Œä¹Ÿæœ‰single-levelä¼˜åŒ–ï¼Œæµå½¢ç»“æ„å»ºæ¨¡ç­‰åæ€å‰æå‡è®¾çš„ç¡¬æ ¸æ–‡ç« ï¼Œæ›´å¤šçš„NAS benchmarkä»¥åŠæ³›åŒ–åº”ç”¨æ¨åŠ¨NASç ”ç©¶çš„è¿›æ­¥ä¸è½åœ°ã€‚è¯šç„¶ï¼Œæ¨¡å‹æœç´¢æ–¹é¢çš„ç ”ç©¶å¤§å¦ä¼¼ä¹å·²ç»è½æˆï¼Œéƒ¨åˆ†æ–°çš„æ”¹è¿›å·¥ä½œå› ä¸ºè¯„æµ‹æ•°æ®é›†ç®€å•(å¦‚cifarç­‰)å’Œä¸å…¬å¹³å¯¹æ¯”è€Œéš¾ä»¥åˆ¤æ–­è‰¯è ï¼ŒæœŸå¾…å¤§ä½¬ç»„ä»¥åŠå¤§å‚èƒ½å¤ŸæŒç»­å¼€å‘ä»¥åŠè´¡çŒ®æ›´å¤šsolidå·¥ä½œã€‚

**ICLR2021ä¸­NASæ–¹é¢æŠ•ç¨¿æ–‡ç« æ•´ç†ï¼š**

**1.** [**How to Train Your Super-Net: An Analysis of Training Heuristics i**](https://openreview.net/forum?id=txC1ObHJ0wB)**n Weight-Sharing NAS**

**2.** [**DARTS-: Robustly Stepping out of Performance Collapse Without Indicators**](https://openreview.net/forum?id=KLH36ELmwIB)

**3.** [**Noisy Differentiable Architecture Search**](https://openreview.net/forum?id=JUgC3lqn6r2) 

**4.** [**FTSO: Effective NAS via First Topology Second Operator**](https://openreview.net/forum?id=7Z29QbHxIL)

Our method, named FTSO, reduces NAS's search time from days to 0.68 seconds while achieving 76.42% testing accuracy on ImageNet and 97.77% testing accuracy on CIFAR10 via searching for network topology and operators separately

**5.** [**DOTS: Decoupling Operation and Topology in Differentiable Architecture Search**](https://openreview.net/forum?id=y6IlNbrKcwG)

We improve DARTS by discoupling the topology representation from the operation weights and make explicit topology search.

**4.** [**Geometry-Aware Gradient Algorithms for Neural Architecture Search**](https://openreview.net/forum?id=MuSYkd1hxRP)

Studying the right single-level optimization geometry yields state-of-the-art methods for NAS.

**5.** [**GOLD-NAS: Gradual, One-Level, Differentiable**](https://openreview.net/forum?id=DsbhGImWjF)

A new differentiable NAS framework incorporating one-level optimization and gradual pruning, working on large search spaces.

**6.** [**Weak NAS Predictor Is All You Need**](https://openreview.net/forum?id=kic8cng35wX)

We present a novel method to estimate weak predictors progressively in predictor-based neural architecture search. By coarse-to-fine iteration, the ranking of sampling space is refined gradually which helps find the optimal architectures eventually.

**7.** [**Differentiable Graph Optimization for Neural Architecture Search**](https://openreview.net/forum?id=NqWY3s0SILo)

we learn a differentiable graph neural network as a surrogate model to rank candidate architectures.

**8 .** [**DrNAS: Dirichlet Neural Architecture Search**](https://openreview.net/forum?id=9FWas6YbmB3) 

we propose a simple yet effective progressive learning scheme that enables searching directly on large-scale tasks, eliminating the gap between search and evaluation phases. Extensive experiments demonstrate the effectiveness of our method. 

**9 .** [**Neural Architecture Search of SPD Manifold Networks**](https://openreview.net/forum?id=1toB0Fo9CZy)

we first introduce a geometrically rich and diverse SPD neural architecture search space for an efficient SPD cell design. Further, we model our new NAS problem using the supernet strategy which models the architecture search problem as a one-shot training process of a single supernet.

**10 .** [**Neighborhood-Aware Neural Architecture Search**](https://openreview.net/forum?id=KBWK5Y92BRh)

We propose a neighborhood-aware formulation for neural architecture search to find flat minima in the search space that can generalize better to new settings.

**11 .** [**A Surgery of the Neural Architecture Evaluators**](https://openreview.net/forum?id=xBoKLdKrZd)

This paper assesses current fast neural architecture evaluators with multiple direct criteria, under controlled settings.

**12 .** [**Exploring single-path Architecture Search ranking correlations**](https://openreview.net/forum?id=J40FkbdldTX)

An empirical study of how several method variations affect the quality of the architecture ranking prediction.

**13 .** [**Neural Architecture Search without Training**](https://openreview.net/forum?id=g4E6SAAvACo)

**14 .** [**Zero-Cost Proxies for Lightweight NAS**](https://openreview.net/forum?id=0cmMMy8J5q)

A single minibatch of data is used to score neural networks for NAS instead of performing full training.

**15 .** [**Improving Zero-Shot Neural Architecture Search with Parameters Scoring**](https://openreview.net/forum?id=4QpDyzCoH01) 

A score can be designed taking into account the jacobian in parameter space, that is highly predictive of final performance in a task.

**16 .** [**Multi-scale Network Architecture Search for Object Detection**](https://openreview.net/forum?id=mo3Uqtnvz_)

**17.** [**Triple-Search: Differentiable Joint-Search of Networks, Precision, and Accelerators**](https://openreview.net/forum?id=OLOr1K5zbDu&noteId=OLOr1K5zbDu)

We propose the Triple-Search framework to jointly search network structure, precision and hardware architecture in a differentiable manner.

**18 .** [**TransNAS-Bench-101: Improving Transferrability and Generalizability of Cross-Task Neural Architecture Search**](https://openreview.net/forum?id=HUd2wQ0j200) 

**19 .** [**Searching for Convolutions and a More Ambitious NAS**](https://openreview.net/forum?id=ascdLuNQY4J) 

**20 .** [**EnTranNAS: Towards Closing the Gap between the Architectures in Search and Evaluation**](https://openreview.net/forum?id=qzqBl_nOeAQ) 

We show how effective dimensionality can shed light on a number of phenomena in modern deep learning including double descent, width-depth trade-offs, and subspace inference, while providing a straightforward and compelling generalization metric.

**21 . Efficient Graph Neural Architecture Search**

 By designing a novel and expressive search space, an efficient one-shot NAS method based on stochastic relaxation and natural gradient is proposed. 

**22 .** [**Searching for Convolutions and a More Ambitious NAS**](https://openreview.net/forum?id=ascdLuNQY4J) 

A general-purpose search space for neural architecture search that enables discovering operations that beat convolutions on image data.

**23 .** [**Exploring single-path Architecture Search ranking correlations**](https://openreview.net/forum?id=J40FkbdldTX)

An empirical study of how several method variations affect the quality of the architecture ranking prediction.

**24 .** [**Network Architecture Search for Domain Adaptation**](https://openreview.net/forum?id=4q8qGBf4Zxb)

**25.** [**HW-NAS-Bench: Hardware-Aware Neural Architecture Search Benchmark**](https://openreview.net/forum?id=_0kaDkv3dVf)

**26.** [**Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective**](https://openreview.net/forum?id=Cnon5ezMHtu)

Our TE-NAS framework analyzes the spectrum of the neural tangent kernel (NTK) and the number of linear regions in the input space, achieving high-quality architecture search while dramatically reducing the search cost to four hours on ImageNet.

**27.** [**Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters**](https://openreview.net/forum?id=67ChnrC0ybo)

Fixing errors in gradient estimation of architectural parameters for stabilizing the DARTS algorithm.

**28.** [**NAS-Bench-301 and the Case for Surrogate Benchmarks for Neural Architecture Search**](https://openreview.net/forum?id=1flmvXGGJaa) 

**29.** [**NASOA: Towards Faster Task-oriented Online Fine-tuning**](https://openreview.net/forum?id=NqPW1ZJjXDJ)

We propose a Neural Architecture Search and Online Adaption framework named NASOA towards a faster task-oriented fine-tuning upon the request of users.

**28.** [**Model-based Asynchronous Hyperparameter and Neural Architecture Search**](https://openreview.net/forum?id=a2rFihIU7i) 

We present a new, asynchronous multi-fidelty Bayesian optimization method to efficiently search for hyperparameters and architectures of neural networks.

**29.** [**Searching for Convolutions and a More Ambitious NAS**](https://openreview.net/forum?id=ascdLuNQY4J)

A general-purpose search space for neural architecture search that enables discovering operations that beat convolutions on image data.

**30.** [**A Gradient-based Kernel Approach for Efficient Network Architecture Search**](https://openreview.net/forum?id=5fJ0qcwBNr0)

We first  formulate these two terms into a unified gradient-based kernel and then select architectures with the largest kernels at initialization as the final networks.  The new approach replaces the expensive "train-then-test'' evaluation paradigm.

**31.** [**Fast MNAS: Uncertainty-aware Neural Architecture Search with Lifelong Learning**](https://openreview.net/forum?id=IPGZ6S3LDdw) 

We proposed FNAS which accelerates standard RL based NAS process by 10x and guarantees better performance on various vision tasks.

**32.** [**Explicit Learning Topology for Differentiable Neural Architecture Search**](https://openreview.net/forum?id=AFm2njNEE1)

**33.** [**NASLib: A Modular and Flexible Neural Architecture Search Library**](https://openreview.net/forum?id=EohGx2HgNsA) 

**34.** [**TransNAS-Bench-101: Improving Transferrability and Generalizability of Cross-Task Neural Architecture Search**](https://openreview.net/forum?id=HUd2wQ0j200)

**35.** [**Rethinking Architecture Selection in Differentiable NAS**](https://openreview.net/forum?id=PKubaeJkw3) 

**36.** [**Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective**](https://openreview.net/forum?id=Cnon5ezMHtu)

**37.** [**Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets**](https://openreview.net/forum?id=rkQuFUmUOg3)

We propose an efficient NAS framework that is trained once on a database consisting of datasets and pretrained networks and can rapidly generate a neural architecture for a novel dataset.

**38.** [**Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels**](https://openreview.net/forum?id=j9Rv7qdXjd)

We propose a NAS method that is sample-efficient, highly performant and interpretable.

**39.** [**AutoHAS: Efficient Hyperparameter and Architecture Search**](https://openreview.net/forum?id=ykCRDlfxmk)

**40.** [**EnTranNAS: Towards Closing the Gap between the Architectures in Search and Evaluation**](https://openreview.net/forum?id=qzqBl_nOeAQ) 

**42.** [**Differentiable Graph Optimization for Neural Architecture Search**](https://openreview.net/forum?id=NqWY3s0SILo) 

we learn a differentiable graph neural network as a surrogate model to rank candidate architectures.

**41.** [**Width transfer: on the (in)variance of width optimization**](https://openreview.net/forum?id=2aw7TEq5jo)

we control the training configurations, i.e., network architectures and training data, for three existing width optimization algorithms and find that the optimized widths are largely transferable across settings. 

**42.** [**NAHAS: Neural Architecture and Hardware Accelerator Search**](https://openreview.net/forum?id=fgpXAu8puGj) 

We propose NAHAS, a latency-driven software/hardware co-optimizer that jointly optimize the design of neural architectures and a mobile edge processor.

**43.** [**Neural Network Surgery: Combining Training with Topology Optimization**](https://openreview.net/forum?id=3JI45wPuReY)

We demonstrate a hybrid approach for combining neural network training with a genetic-algorithm based architecture optimization.

**44.** [**Efficient Architecture Search for Continual Learning**](https://openreview.net/forum?id=uUX49ez8P06)

Our proposed CLEAS works closely with neural architecture search (NAS) which leverages reinforcement learning techniques to search for the best neural architecture that fits a new task.

**45.** [**Auto Seg-Loss: Searching Metric Surrogates for Semantic Segmentation**](https://openreview.net/forum?id=MJAqnaC2vO1)

Auto Seg-Loss is the first general framework for searching surrogate losses for mainstream semantic segmentation metrics. 

**46.** [**Improving Random-Sampling Neural Architecture Search by Evolving the Proxy Search Space**](https://openreview.net/forum?id=qk0FE399OJ)

**47.** [**SEDONA: Search for Decoupled Neural Networks toward Greedy Block-wise Learning**](https://openreview.net/forum?id=XLfdzwNKzch)

Our approach is the first attempt to automate decoupling neural networks for greedy block-wise learning and outperforms both end-to-end backprop and state-of-the-art greedy-learning methods on CIFAR-10, Tiny-ImageNet and ImageNet classification.

**48.** [**Intra-layer Neural Architecture Search**](https://openreview.net/forum?id=510f7KAPmYR) 

Neural architecture search at the level of individual weight parameters.
